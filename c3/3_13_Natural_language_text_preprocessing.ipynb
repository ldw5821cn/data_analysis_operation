{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import pandas as pd\n",
    "import jieba  # 结巴分词\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # 基于TF-IDF的词频转向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词函数\n",
    "def jieba_cut(string):\n",
    "    word_list = []  # 建立空列表用于存储分词结果\n",
    "    seg_list = jieba.cut(string)  # 精确模式分词\n",
    "    for word in seg_list:  # 循环读取每个分词\n",
    "        word_list.append(word)  # 分词追加到列表\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = open('text.txt')\n",
    "string_lines = fn.readlines()\n",
    "fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/x1/h5vsy5496674zbr1smgjc9dh0000gn/T/jieba.cache\n",
      "Loading model cost 0.854 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对于\n",
      "数据\n",
      "化\n",
      "运营\n",
      "和\n",
      "   python    上    下        不断   不曾   专业         业   业务   两个        严峻 ...   \\\n",
      "0     0.0  0.0  0.0  0.204648  0.0  0.0  0.204648  0.0  0.0  0.204648 ...    \n",
      "\n",
      "     非   非常  非常简单   预测   领域    高   高于    （    ）    ；  \n",
      "0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1 rows x 198 columns]\n"
     ]
    }
   ],
   "source": [
    "# 中文分词\n",
    "seg_list = []  # 建立空列表，用于存储所有分词结果\n",
    "for string_line in string_lines:  # 读取每行数据\n",
    "    each_list = jieba_cut(string_line)  # 返回每行的分词结果\n",
    "    seg_list.append(each_list)  # 分词结果添加到结果列表\n",
    "for i in range(5):  # 打印输出第一行的前5条数据\n",
    "    print (seg_list[1][i])\n",
    "\n",
    "# word to vector\n",
    "stop_words = [u'\\n', u'/', u'“', u'”', u'的', u'，', u'和', u'是', u'随着', u'对于', u'对', u'等', u'能', u'都', u'。', u'、',\n",
    "              u'中', u'与', u'在', u'其']  # 自定义要去除的无用词\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, tokenizer=jieba_cut)  # 创建词向量模型\n",
    "X = vectorizer.fit_transform(string_lines)  # 将文本数据转换为向量空间模型\n",
    "vector = vectorizer.get_feature_names()  # 获得词向量\n",
    "vector_value = X.toarray()  # 获得词向量值\n",
    "vector_pd = pd.DataFrame(vector_value, columns=vector)  # 创建用于展示的数据框\n",
    "print (vector_pd.head(1))  # 打印输出第一条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

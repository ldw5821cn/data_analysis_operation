{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liudawei/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/liudawei/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# 导入库\n",
    "import tarfile  # tar压缩包库\n",
    "import os  # 操作系统功能模块\n",
    "import jieba.posseg as pseg  # 带词性标注的分词模块\n",
    "from gensim import corpora, models  # gensim的词频统计和主题建模模块\n",
    "from bs4 import BeautifulSoup  # 用于XML格式化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分词\n",
    "def jieba_cut(text):\n",
    "    '''\n",
    "    将输入的文本句子根据词性标注做分词\n",
    "    :param text: 文本句子，字符串型\n",
    "    :return: 符合规则的分词结果\n",
    "    '''\n",
    "    rule_words = ['z', 'vn', 'v', 't', 'nz', 'nr', 'ns', 'n', 'l', 'i', 'j', 'an',\n",
    "                  'a']  # 只保留状态词、名动词、动词、时间词、其他名词、人名、地名、名词、习用语、简称略语、成语、形容词、名形词\n",
    "    words = pseg.cut(text)  # 分词\n",
    "    seg_list = []  # 列表用于存储每个文件的分词结果\n",
    "    for word in words:  # 循环得到每个分词\n",
    "        if word.flag in rule_words:\n",
    "            seg_list.append(word.word)  # 将分词追加到列表\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "def text_pro(words_list, tfidf_object=None, training=True):\n",
    "    '''\n",
    "    gensim主题建模预处理过程，包含分词类别转字典、生成语料库和TF-IDF转换\n",
    "    :param words_list: 分词列表，列表型\n",
    "    :param tfidf_object: TF-IDF模型对象，该对象在训练阶段生成\n",
    "    :param training: 是否训练阶段，用来针对训练和预测两个阶段做预处理\n",
    "    :return: 如果是训练阶段，返回词典、TF-IDF对象和TF-IDF向量空间数据；如果是预测阶段，返回TF-IDF向量空间数据\n",
    "    '''\n",
    "    # 分词列表转字典\n",
    "    dic = corpora.Dictionary(words_list)  # 将分词列表转换为字典形式\n",
    "    print ('{:*^60}'.format('token & word mapping review:'))\n",
    "    itemss = dic.items()\n",
    "    print(itemss[:,5])\n",
    "    for i, w in itemss:  # 循环读出字典前5条的每个key和value，对应的是索引值和分词\n",
    "        print ('token:%s -- word:%s' % (i, w))\n",
    "    # 生成语料库\n",
    "    corpus = []  # 建立一个用于存储语料库的列表\n",
    "    for words in words_list:  # 读取每个分词列表\n",
    "        corpus.append(dic.doc2bow(words))  # 将每个分词列表转换为语料库词袋（bag of words）形式的列表\n",
    "    print ('{:*^60}'.format('bag of words review:'))\n",
    "    print (corpus[0])  # 打印输出第一条语料库\n",
    "    # TF-IDF转换\n",
    "    if training == True:\n",
    "        tfidf = models.TfidfModel(corpus)  # 建立TF-IDF模型对象\n",
    "        corpus_tfidf = tfidf[corpus]  # 得到TF-IDF向量稀疏矩阵\n",
    "        print ('{:*^60}'.format('TF-IDF model review:'))\n",
    "        for doc in corpus_tfidf:  # 循环读出每个向量\n",
    "            print(doc)  # 打印第一条向量\n",
    "            break  # 跳出循环\n",
    "        return dic, corpus_tfidf, tfidf\n",
    "    else:\n",
    "        return tfidf_object[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全角转半角\n",
    "def str_convert(content):\n",
    "    '''\n",
    "    将内容中的全角字符，包含英文字母、数字键、符号等转换为半角字符\n",
    "    :param content: 要转换的字符串内容\n",
    "    :return: 转换后的半角字符串\n",
    "    '''\n",
    "    new_str = ''\n",
    "    for each_char in content:  # 循环读取每个字符\n",
    "        code_num = ord(each_char)  # 读取字符的ASCII值或Unicode值\n",
    "        if code_num == 12288:  # 全角空格直接转换\n",
    "            code_num = 32\n",
    "        elif (code_num >= 65281 and code_num <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "            code_num -= 65248\n",
    "        new_str += chr(code_num)\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析文件内容\n",
    "def data_parse(data):\n",
    "    '''\n",
    "    从原始文件中解析出文本内容数据\n",
    "    :param data: 包含代码的原始内容\n",
    "    :return: 文本中的所有内容，列表型\n",
    "    '''\n",
    "    raw_code = BeautifulSoup(data, \"lxml\")  # 建立BeautifulSoup对象\n",
    "    content_code = raw_code.find_all('content')  # 从包含文本的代码块中找到content标签\n",
    "    content_list = []  # 建立空列表，用来存储每个content标签的内容\n",
    "    for each_content in content_code:  # 循环读出每个content标签\n",
    "        if len(each_content) > 0:  # 如果content标签的内容不为空\n",
    "            raw_content = each_content.text  # 获取原始内容字符串\n",
    "            convert_content = str_convert(raw_content)  # 将全角转换为半角\n",
    "            content_list.append(convert_content)  # 将content文本内容加入列表\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压缩文件\n",
    "if not os.path.exists('./news_data'):  # 如果不存在数据目录，则先解压数据文件\n",
    "    print ('extract data from news_data.tar.gz...')\n",
    "    tar = tarfile.open('news_data.tar.gz')  # 打开tar.gz压缩包对象\n",
    "    names = tar.getnames()  # 获得压缩包内的每个文件对象的名称\n",
    "    for name in names:  # 循环读出每个文件\n",
    "        tar.extract(name, path='./')  # 将文件解压到指定目录\n",
    "    tar.close()  # 关闭压缩包对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk files and get content...\n"
     ]
    }
   ],
   "source": [
    "# 汇总所有内容\n",
    "print('walk files and get content...')\n",
    "all_content = []  # 总列表，用于存储所有文件的文本内容\n",
    "for root, dirs, files in os.walk('./news_data'):  # 分别读取遍历目录下的根目录、子目录和文件列表\n",
    "    for file in files:  # 读取每个文件\n",
    "        file_name = os.path.join(root, file)  # 将目录路径与文件名合并为带有完整路径的文件名\n",
    "        with open(file_name) as f:  # 以只读方式打开文件\n",
    "            data = f.read()  # 读取文件内容\n",
    "        all_content.extend(data_parse(data))  # 从文件内容中获取文本并将结果追加到总列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get word list...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0a47bbe138ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 分词列表，用于存储所有文件的分词结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach_content\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_content\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 循环读出每个文本内容\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mwords_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjieba_cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 将文件内容的分词结果以列表的形式追加到列表\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-99f5f664cff3>\u001b[0m in \u001b[0;36mjieba_cut\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 分词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mseg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 列表用于存储每个文件的分词结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 循环得到每个分词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrule_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mseg_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 将分词追加到列表\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jieba/posseg/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(sentence, HMM)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHMM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHMM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jieba/posseg/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(self, sentence, HMM)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHMM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cut_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHMM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHMM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jieba/posseg/__init__.py\u001b[0m in \u001b[0;36m__cut_internal\u001b[0;34m(self, sentence, HMM)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mre_han_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcut_blk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jieba/posseg/__init__.py\u001b[0m in \u001b[0;36m__cut_DAG\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__cut_DAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mDAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_DAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mroute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mget_DAG\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mfrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mN\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mtmplist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 获取每条内容的分词结果\n",
    "print ('get word list...')\n",
    "words_list = []  # 分词列表，用于存储所有文件的分词结果\n",
    "for each_content in all_content:  # 循环读出每个文本内容\n",
    "    words_list.append(list(jieba_cut(each_content)))  # 将文件内容的分词结果以列表的形式追加到列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立主题模型\n",
    "print ('train topic model...')\n",
    "dic, corpus_tfidf, tfidf = text_pro(words_list, tfidf_object=None, training=True)  # 训练集的文本预处理\n",
    "num_topics = 3  # 设置主题个数\n",
    "lda = models.LdaModel(corpus_tfidf, id2word=dic, num_topics=num_topics)  # 通过LDA进行主题建模\n",
    "print ('{:*^60}'.format('topic model review:'))\n",
    "for i in range(num_topics):  # 输出每一类主题的结果\n",
    "    print (lda.print_topic(i))  # 输出对应主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新数据集的主题模型预测\n",
    "print ('topic forecast...')\n",
    "with open('article.txt') as f:  # 打开新的文本\n",
    "    text_new = f.read()  # 读取文本数据\n",
    "text_content = data_parse(data)  # 解析新的文本\n",
    "words_list_new = jieba_cut(text_new)  # 将文本转换为分词列表\n",
    "corpus_tfidf_new = text_pro([words_list_new], tfidf_object=tfidf, training=False)  # 新文本数据集的预处理\n",
    "corpus_lda_new = lda[corpus_tfidf_new]  # 获取新的分词列表（文档）的主题概率分布\n",
    "print ('{:*^60}'.format('topic forecast:'))\n",
    "print (list(corpus_lda_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
